%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{IDL\_Guide Documentation}
\date{Dec 02, 2019}
\release{0.9}
\author{us}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


This package can be used to fit an Implicit Deep Learning (IDL)
model for regression and classification purpose.


\chapter{Introduction}
\label{\detokenize{sections/introduction:introduction}}\label{\detokenize{sections/introduction::doc}}
This package can be used to fit an Implicit Deep Learning (IDL) model for regression
and classification purpose.

The IDL.fit function estimates a vector of parameters by applying successively
gradient descents (see more at {\hyperref[\detokenize{sections/gradient_descents:gradient-descents}]{\sphinxcrossref{\DUrole{std,std-ref}{Gradient Descents}}}}) and dual ascent
(see more at {\hyperref[\detokenize{sections/dual_ascents:dual-ascents}]{\sphinxcrossref{\DUrole{std,std-ref}{Dual Ascents}}}}).


\section{Implicit Deep Learning}
\label{\detokenize{sections/introduction:id1}}
** section 1.1 **

Given an input \(u \in \mathbb{R}^n\), where n denotes the number of features,
we define the implicit deep learning prediction rule \(\hat{y}(u) \in \mathbb{R}^n\) with ReLU activation
\begin{equation*}
\begin{split}\hat{y}(u) = ...\end{split}
\end{equation*}

\section{Loss Functions}
\label{\detokenize{sections/introduction:loss-functions}}
** section 3.2 equation 4 ** + Classification loss
(see more at {\hyperref[\detokenize{sections/learning:learning}]{\sphinxcrossref{\DUrole{std,std-ref}{Learning Process}}}})


\section{Description of the learning process}
\label{\detokenize{sections/introduction:description-of-the-learning-process}}
(see more at {\hyperref[\detokenize{sections/bi_convex_formulation:formulation}]{\sphinxcrossref{\DUrole{std,std-ref}{Formulation for IDL}}}})


\section{Description of the prediction process}
\label{\detokenize{sections/introduction:description-of-the-prediction-process}}
(see more at {\hyperref[\detokenize{sections/prediction:prediction}]{\sphinxcrossref{\DUrole{std,std-ref}{Predicting}}}})


\section{Setup}
\label{\detokenize{sections/introduction:setup}}
TODO

The package is compatible with Python version 3 or higher only.
The user is expected to have installed cvxpy before running the package.
Go to … for more information.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Switch to a proper directory and then type:

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{o}{+} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}


\chapter{Formulation for IDL}
\label{\detokenize{sections/bi_convex_formulation:formulation-for-idl}}\label{\detokenize{sections/bi_convex_formulation:formulation}}\label{\detokenize{sections/bi_convex_formulation::doc}}
See {\hyperref[\detokenize{sections/citing:citing}]{\sphinxcrossref{\DUrole{std,std-ref}{Citing}}}} for in-depth explanation


\section{Problem Formulation}
\label{\detokenize{sections/bi_convex_formulation:problem-formulation}}
** section 3.1 **


\section{Fenchel Divergence Lagrangian Relaxation}
\label{\detokenize{sections/bi_convex_formulation:fenchel-divergence-lagrangian-relaxation}}
** section 3.2 **


\section{Linear matrix inequality}
\label{\detokenize{sections/bi_convex_formulation:linear-matrix-inequality}}
** section 3.3 **


\section{Bi-convex Formulation}
\label{\detokenize{sections/bi_convex_formulation:module-utilities.GradientDescents}}\label{\detokenize{sections/bi_convex_formulation:bi-convex-formulation}}\index{utilities.GradientDescents (module)@\spxentry{utilities.GradientDescents}\spxextra{module}}\index{gradient\_descent\_theta() (in module utilities.GradientDescents)@\spxentry{gradient\_descent\_theta()}\spxextra{in module utilities.GradientDescents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{sections/bi_convex_formulation:utilities.GradientDescents.gradient_descent_theta}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utilities.GradientDescents.}}\sphinxbfcode{\sphinxupquote{gradient\_descent\_theta}}}{\emph{theta}, \emph{X}, \emph{U}, \emph{Y}}{}
Returns the gradient of theta
:param theta: a dictionary
:param X: hidden variables
:param U: input data
:param Y: output data
:return: grad\_theta: dictionary containing gradients of elemnts in theta

\end{fulllineitems}



\chapter{Learning Process}
\label{\detokenize{sections/learning:learning-process}}\label{\detokenize{sections/learning:learning}}\label{\detokenize{sections/learning::doc}}
** section 3.4 algo in the end **
\index{IDLModel (class in IDL)@\spxentry{IDLModel}\spxextra{class in IDL}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{sections/learning:IDL.IDLModel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{IDL.}}\sphinxbfcode{\sphinxupquote{IDLModel}}}{\emph{hidden\_features=1}, \emph{alpha=0.1}, \emph{epsilon=0.1}, \emph{random\_state=0}, \emph{seed=None}}{}~\begin{description}
\item[{demo\_param}] \leavevmode{[}str, default=’demo\_param’{]}
A parameter used for demonstation of how to pass and store paramters.

\end{description}
\index{fit() (IDL.IDLModel method)@\spxentry{fit()}\spxextra{IDL.IDLModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{sections/learning:IDL.IDLModel.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{X}, \emph{y}, \emph{verbose=1}, \emph{rounds\_number=100}, \emph{sample\_weight=None}, \emph{eval\_set=None}, \emph{eval\_metric=None}, \emph{early\_stopping\_rounds=None}, \emph{callbacks=None}}{}
Fit the IDLModel parameters
\begin{description}
\item[{X}] \leavevmode{[}\{array-like, sparse matrix\}, shape (m\_samples,  n\_features){]}
The training input samples.

\item[{y}] \leavevmode{[}array-like, shape (m\_samples, ) or (m\_samples, p\_outputs){]}
The target values (class labels in classification, real numbers in
regression).

\end{description}

verbose: If True (1) then we print everything on the console
rounds\_number : how many rounds max do you want to do
early\_stopping : If True : once the L2Loss will increase we automatically stop
sample\_weight : array\_like
\begin{quote}

instance weights
\end{quote}
\begin{description}
\item[{eval\_set}] \leavevmode{[}list, optional{]}
A list of (X, y) tuple pairs to use as a validation set for
early-stopping

\item[{eval\_metric}] \leavevmode{[}str, callable, optional{]}
If a str, should be a built-in evaluation metric to use. See
doc/parameter.rst. If callable, a custom evaluation metric. The call
signature is func(y\_predicted, y\_true) where y\_true will be a
DMatrix object such that you may need to call the get\_label
method. It must return a str, value pair where the str is a name
for the evaluation and value is the value of the evaluation
function. This objective is always minimized.

\item[{early\_stopping\_rounds}] \leavevmode{[}int{]}
Activates early stopping. Validation error needs to decrease at
least every \textless{}early\_stopping\_rounds\textgreater{} round(s) to continue training.
Requires at least one item in evals.  If there’s more than one,
will use the last. Returns the model from the last iteration
(not the best one). If early stopping occurs, the model will
have three additional fields: bst.best\_score, bst.best\_iteration
and bst.best\_ntree\_limit.
(Use bst.best\_ntree\_limit to get the correct value if num\_parallel\_tree
and/or num\_class appears in the parameters)

\end{description}
\begin{description}
\item[{self}] \leavevmode{[}object{]}
Returns self.

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Gradient Descents}
\label{\detokenize{sections/gradient_descents:gradient-descents}}\label{\detokenize{sections/gradient_descents:id1}}\label{\detokenize{sections/gradient_descents::doc}}

\section{Bi-Convexity of the Loss function}
\label{\detokenize{sections/gradient_descents:bi-convexity-of-the-loss-function}}
TODO


\section{Gradient Descents}
\label{\detokenize{sections/gradient_descents:id2}}
TODO

\phantomsection\label{\detokenize{sections/gradient_descents:module-utilities.GradientDescents}}\index{utilities.GradientDescents (module)@\spxentry{utilities.GradientDescents}\spxextra{module}}\index{gradient\_descent\_theta() (in module utilities.GradientDescents)@\spxentry{gradient\_descent\_theta()}\spxextra{in module utilities.GradientDescents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{sections/gradient_descents:utilities.GradientDescents.gradient_descent_theta}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utilities.GradientDescents.}}\sphinxbfcode{\sphinxupquote{gradient\_descent\_theta}}}{\emph{theta}, \emph{X}, \emph{U}, \emph{Y}}{}
Returns the gradient of theta
:param theta: a dictionary
:param X: hidden variables
:param U: input data
:param Y: output data
:return: grad\_theta: dictionary containing gradients of elemnts in theta

\end{fulllineitems}



\chapter{Dual Ascents}
\label{\detokenize{sections/dual_ascents:dual-ascents}}\label{\detokenize{sections/dual_ascents:id1}}\label{\detokenize{sections/dual_ascents::doc}}\phantomsection\label{\detokenize{sections/dual_ascents:module-utilities.DualAscents}}\index{utilities.DualAscents (module)@\spxentry{utilities.DualAscents}\spxextra{module}}

\chapter{Predicting}
\label{\detokenize{sections/prediction:predicting}}\label{\detokenize{sections/prediction:prediction}}\label{\detokenize{sections/prediction::doc}}
** section 2 Picard iterations **
\index{IDLModel (class in IDL)@\spxentry{IDLModel}\spxextra{class in IDL}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{sections/prediction:IDL.IDLModel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{IDL.}}\sphinxbfcode{\sphinxupquote{IDLModel}}}{\emph{hidden\_features=1}, \emph{alpha=0.1}, \emph{epsilon=0.1}, \emph{random\_state=0}, \emph{seed=None}}{}~\begin{description}
\item[{demo\_param}] \leavevmode{[}str, default=’demo\_param’{]}
A parameter used for demonstation of how to pass and store paramters.

\end{description}
\index{predict() (IDL.IDLModel method)@\spxentry{predict()}\spxextra{IDL.IDLModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{sections/prediction:IDL.IDLModel.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{X}}{}
A reference implementation of a predicting function.
\begin{description}
\item[{X}] \leavevmode{[}\{array-like, sparse matrix\}, shape (n\_samples, n\_features){]}
The training input samples.

\end{description}
\begin{description}
\item[{y}] \leavevmode{[}ndarray, shape (n\_samples,){]}
Returns an array of ones.

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Examples}
\label{\detokenize{sections/classification_operational_example:examples}}\label{\detokenize{sections/classification_operational_example::doc}}

\section{\sphinxstylestrong{Classification : MNIST}}
\label{\detokenize{sections/classification_operational_example:classification-mnist}}

\section{\sphinxstylestrong{Regression : Boston Housing}}
\label{\detokenize{sections/classification_operational_example:regression-boston-housing}}

\chapter{Citing}
\label{\detokenize{sections/citing:citing}}\label{\detokenize{sections/citing:id1}}\label{\detokenize{sections/citing::doc}}\begin{itemize}
\item {} 
“Implicit Deep Learning” Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, arXiv:1908.06315, 2019.

\item {} 
“Bi-convex Implicit Deep Learning” Bertrand Travacca, October 2019

\end{itemize}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{u}
\item\relax\sphinxstyleindexentry{utilities.DualAscents}\sphinxstyleindexpageref{sections/dual_ascents:\detokenize{module-utilities.DualAscents}}
\item\relax\sphinxstyleindexentry{utilities.GradientDescents}\sphinxstyleindexpageref{sections/gradient_descents:\detokenize{module-utilities.GradientDescents}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}