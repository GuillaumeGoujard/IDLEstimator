
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap-theme.min.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
    <title>Gradient Descents &#8212; IDL 0.9 documentation</title>
    <link rel="stylesheet" href="../_static/guzzle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dual Ascents" href="dual_ascents.html" />
    <link rel="prev" title="Learning Process" href="learning.html" />
  
   

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="dual_ascents.html" title="Dual Ascents"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="learning.html" title="Learning Process"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">IDL 0.9 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column">
    <div class="sphinxsidebar">
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <h2>Contents</h2>
    <div class="sidebar-localtoc">
      <ul>
<li><a class="reference internal" href="#">Gradient Descents</a><ul>
<li><a class="reference internal" href="#block-coordinate-descent-and-first-order-methods">Block coordinate descent and first order methods</a></li>
<li><a class="reference internal" href="#bi-convexity-of-the-loss-function">Bi-Convexity of the Loss function</a></li>
<li><a class="reference internal" href="#id4">Gradient Descents</a></li>
<li><a class="reference internal" href="#code-for-calculating-gradient-descent">Code for calculating Gradient Descent</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
</div>
  <h4>Previous topic</h4>
  <p class="topless"><a href="learning.html"
                        title="previous chapter">Learning Process</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="dual_ascents.html"
                        title="next chapter">Dual Ascents</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/sections/gradient_descents.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <div id="main-search">
      <form class="form-inline" action="../search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
      
    </div>
  </div>
        <div id="right-column">
          
          <div role="navigation" aria-label="breadcrumbs navigation">
            <ol class="breadcrumb">
              <li><a href="../index.html">Docs</a></li>
              
              <li>Gradient Descents</li>
            </ol>
          </div>
          
          <div class="document clearer body">
            
  <div class="section" id="gradient-descents">
<span id="id1"></span><h1>Gradient Descents<a class="headerlink" href="#gradient-descents" title="Permalink to this headline">¶</a></h1>
<div class="section" id="block-coordinate-descent-and-first-order-methods">
<h2>Block coordinate descent and first order methods<a class="headerlink" href="#block-coordinate-descent-and-first-order-methods" title="Permalink to this headline">¶</a></h2>
<p>As problem <a class="reference internal" href="bi_convex_formulation.html#equation-eq-7">(6)</a> is bi-convex, a natural strategy is the use of block coordinate descent (BCD): alternating
optimization between <span class="math notranslate nohighlight">\(\Theta\)</span>. BCD corresponds to the following algorithm,</p>
<p><strong>for</strong> <span class="math notranslate nohighlight">\(k = 1, 2, \cdots\)</span> <strong>do</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \Theta^k \in \text{argmin}_{\Theta} &amp;\frac{1}{2m} \Vert AX^{k-1} + BU + c1_m^T - Y \Vert_F^2 + \lambda^T \mathcal{F}(X^{k-1},DX^{k-1} + EU + f1_m^T) \\
        &amp;\quad \Lambda + A^TA - (\Lambda D + D^T \Lambda) \in \mathbb{S}_+^h, \\
        &amp;\quad \Vert D \Vert_2 \leq 1 - \epsilon \\
    X^k \in \text{argmin}_{X \geq 0} &amp;\frac{1}{2m}\Vert A^kX + B^kU + c^k1_m^T - Y \Vert_F^2 + \lambda^T \mathcal{F}(X,D^kX + E^kU + f^k1_m^T)
\end{align}\end{split}\]</div>
<p><strong>end</strong></p>
<p>In practice such updates might be to heavy computationally as the number of datapoints <span class="math notranslate nohighlight">\(m\)</span> increase, or
as the model size increases (i.e. <span class="math notranslate nohighlight">\(h\)</span>, <span class="math notranslate nohighlight">\(n\)</span> or <span class="math notranslate nohighlight">\(p\)</span>). Instead we propose to do block coordinate projected gradient
updates. This method is also considered to be better at avoiding local minima. Let us denote</p>
<div class="math notranslate nohighlight">
\[\mathcal{G}(\Theta,X) := \mathcal{L}(Y,[\Theta,X]) + \lambda^T \mathcal{F}(X,DX + EU + f1_m^T)\]</div>
<p>In the remainder of this section we derive the gradients <span class="math notranslate nohighlight">\(\nabla_{\Theta} \mathcal{G}(\Theta,X), \nabla_X \mathcal{G}(\Theta,X)\)</span> and corresponding ‘optimal’
step-sizes using the Lipschitz coefficients of the gradients- which is the advantage of having a bi-smooth
optimization problem. Note that the objective <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, given <span class="math notranslate nohighlight">\(X\)</span> is separable in <span class="math notranslate nohighlight">\(\Theta_1 := (A,B,c)\)</span> and <span class="math notranslate nohighlight">\(\Theta_2 := (D,E,f)\)</span>.
Using scalar by matrix calculus</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
        \nabla_A \mathcal{G}(\Theta,X) = \Omega(A,B,c)X^T \in \mathbb{R}^{p \times h} \\
        \nabla_B \mathcal{G}(\Theta,X) = \Omega(A,B,c)U^T \in \mathbb{R}^{p \times n} \\
\nabla_c \mathcal{G}(\Theta,X) = \Omega(A,B,c)1_m \in \mathbb{R}^p
\end{cases},\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(\Omega(A,B,c) := \frac{1}{m}(AX + BU + c1_m^T - Y) \in \mathbb{R}^{p \times m}\)</span>. Hence we can show that a Lipschitz constant for the
gradient is given by</p>
<div class="math notranslate nohighlight">
\[L_{\Theta_1}(X) := \frac{1}{m} \max(m,\Vert X \Vert_2^2,\Vert U \Vert_2^2,\Vert XU^T \Vert_2),\]</div>
<p>and the ‘optimal’ step-size for gradient descent is then simply given by</p>
<div class="math notranslate nohighlight">
\[\alpha_{\Theta_1}(X) := \frac{1}{L_{\Theta}(X)}.\]</div>
<p>Regarding the gradient with respect to <span class="math notranslate nohighlight">\(\Theta_2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
        \nabla_D \mathcal{G}(\Theta,X) = \Omega(D,E,f,\Lambda)X^T \in \mathbb{R}^{h \times h} \\
        \nabla_E \mathcal{G}(\Theta,X) = \Omega(D,E,f,\Lambda)U^T \in \mathbb{R}^{h \times n} \\
\nabla_f \mathcal{G}(\Theta,X) = \Omega(D,E,f,\Lambda)1_m \in \mathbb{R}^h
\end{cases},\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(\Omega(D,E,f,\Lambda) := \frac{\Lambda}{m}\bigg((DX + EU + f1_m^T)_+ - X \bigg) \in \mathbb{R}^{h \times m}\)</span>, we can show that a Lipschitz constant for the
gradient is</p>
<div class="math notranslate nohighlight">
\[L_{\Theta_2}(X) := \frac{\lambda_{\text{max}}}{m} \max(m,\Vert X \Vert_2^2,\Vert U \Vert_2^2,\Vert X \Vert_2 \Vert U \Vert_2),\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_{\text{max}} = \text{max}_{j \in \{1,\cdots,h\}} \lambda_j\)</span>. We can then similarly define an ‘optimal’ step-size <span class="math notranslate nohighlight">\(\alpha \Theta_2\)</span>.
We have that</p>
<div class="math notranslate nohighlight">
\[\nabla_X \mathcal{G}(\Theta,X) = \frac{1}{m} \bigg\{ A^T(AX + BU + c1_m^T) + (\Lambda - \Lambda D - D^T \Lambda)X + D^T \Lambda (DX + EU + f1_m^T)_+ - \Lambda(EU+f1_m^T) \bigg\}.\]</div>
<p>A Lipschitz constant for this gradient is</p>
<div class="math notranslate nohighlight">
\[L_X(\Theta) = \frac{1}{m}(\Vert A^TA + \Lambda - \Lambda D + D^T\Lambda \Vert_2 + \lambda_{\text{max}} \Vert D \Vert_2^2).\]</div>
<p>We can then take the step-size <span class="math notranslate nohighlight">\(\alpha_X(\Theta) = \frac{1}{L_X(\Theta)}\)</span>. We propose the following block coordinate projected
gradient scheme (BC-gradient) to  nd a candidate solution to :eq:<a href="#id2"><span class="problematic" id="id3">`</span></a>eq_7. We denote compactly the convex set</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}_{\Theta} := \{\Theta \vert \Lambda + A^TA - (\Lambda D + D^T \Lambda) \in \mathbb{s}_+^h, \Vert D \Vert_2 \leq 1 - \epsilon \}\]</div>
<p>and <span class="math notranslate nohighlight">\(\mathcal{P}_{\mathcal{S}_{\Theta}}\)</span> the corresponding convex projection</p>
<p><strong>for</strong> <span class="math notranslate nohighlight">\(k = 1, 2, \cdots\)</span> <strong>do</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \Theta^k &amp;= \mathcal{P}_{\mathcal{S}_{\Theta}}\bigg(\Theta^k - \alpha_{\Theta}(X^{k-1}) \nabla_{\Theta} \mathcal{G}(\Theta^{k-1},X^{k-1}) \bigg) \\
    X^k &amp;= \bigg(X^{k-1} - \alpha_X(\Theta^k) \nabla_X \mathcal{G}(\Theta^k,X^{k-1}) \bigg)
\end{align}\end{split}\]</div>
<p><strong>end</strong></p>
</div>
<div class="section" id="bi-convexity-of-the-loss-function">
<h2>Bi-Convexity of the Loss function<a class="headerlink" href="#bi-convexity-of-the-loss-function" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
<div class="section" id="id4">
<h2>Gradient Descents<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
<div class="section" id="code-for-calculating-gradient-descent">
<h2>Code for calculating Gradient Descent<a class="headerlink" href="#code-for-calculating-gradient-descent" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-utilities.GradientDescents"></span><dl class="function">
<dt id="utilities.GradientDescents.gradient_descent_theta">
<code class="sig-prename descclassname">utilities.GradientDescents.</code><code class="sig-name descname">gradient_descent_theta</code><span class="sig-paren">(</span><em class="sig-param">theta</em>, <em class="sig-param">X</em>, <em class="sig-param">U</em>, <em class="sig-param">Y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utilities/GradientDescents.html#gradient_descent_theta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utilities.GradientDescents.gradient_descent_theta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the gradient of theta</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – a dictionary</p></li>
<li><p><strong>X</strong> – hidden variables</p></li>
<li><p><strong>U</strong> – input data</p></li>
<li><p><strong>Y</strong> – output data</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>grad_theta: dictionary containing gradients of elemnts in theta</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


          </div>
            
  <div class="footer-relations">
    
      <div class="pull-left">
        <a class="btn btn-default" href="learning.html" title="previous chapter (use the left arrow)">Learning Process</a>
      </div>
    
      <div class="pull-right">
        <a class="btn btn-default" href="dual_ascents.html" title="next chapter (use the right arrow)">Dual Ascents</a>
      </div>
    </div>
    <div class="clearer"></div>
  
        </div>
        <div class="clearfix"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="dual_ascents.html" title="Dual Ascents"
             >next</a> |</li>
        <li class="right" >
          <a href="learning.html" title="Learning Process"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">IDL 0.9 documentation</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="../_static/js/bootstrap.js"></script>
  <div class="footer">
    &copy; Copyright . Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>