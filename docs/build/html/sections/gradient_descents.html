
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Gradient Descents &#8212; IDL 0.9 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dual Ascents" href="dual_ascents.html" />
    <link rel="prev" title="Learning Process" href="learning.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="gradient-descents">
<span id="id1"></span><h1>Gradient Descents<a class="headerlink" href="#gradient-descents" title="Permalink to this headline">¶</a></h1>
<div class="section" id="block-coordinate-descent-and-first-order-methods">
<h2>Block coordinate descent and first order methods<a class="headerlink" href="#block-coordinate-descent-and-first-order-methods" title="Permalink to this headline">¶</a></h2>
<p>As problem <a class="reference internal" href="bi_convex_formulation.html#equation-eq-7">(6)</a> is bi-convex, a natural strategy is the use of block coordinate descent (BCD): alternating
optimization between <span class="math notranslate nohighlight">\(\Theta\)</span>. BCD corresponds to the following algorithm,</p>
<p><strong>for</strong> <span class="math notranslate nohighlight">\(k = 1, 2, \cdots\)</span> <strong>do</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \Theta^k \in \text{argmin}_{\Theta} &amp;\frac{1}{2m} \Vert AX^{k-1} + BU + c1_m^T - Y \Vert_F^2 + \lambda^T \mathcal{F}(X^{k-1},DX^{k-1} + EU + f1_m^T) \\
        &amp;\quad \Lambda + A^TA - (\Lambda D + D^T \Lambda) \in \mathbb{S}_+^h, \\
        &amp;\quad \Vert D \Vert_2 \leq 1 - \epsilon \\
    X^k \in \text{argmin}_{X \geq 0} &amp;\frac{1}{2m}\Vert A^kX + B^kU + c^k1_m^T - Y \Vert_F^2 + \lambda^T \mathcal{F}(X,D^kX + E^kU + f^k1_m^T)
\end{align}\end{split}\]</div>
<p><strong>end</strong></p>
<p>In practice such updates might be to heavy computationally as the number of datapoints <span class="math notranslate nohighlight">\(m\)</span> increase, or
as the model size increases (i.e. <span class="math notranslate nohighlight">\(h\)</span>, <span class="math notranslate nohighlight">\(n\)</span> or <span class="math notranslate nohighlight">\(p\)</span>). Instead we propose to do block coordinate projected gradient
updates. This method is also considered to be better at avoiding local minima. Let us denote</p>
<div class="math notranslate nohighlight">
\[\mathcal{G}(\Theta,X) := \mathcal{L}(Y,[\Theta,X]) + \lambda^T \mathcal{F}(X,DX + EU + f1_m^T)\]</div>
<p>In the remainder of this section we derive the gradients <span class="math notranslate nohighlight">\(\nabla_{\Theta} \mathcal{G}(\Theta,X), \nabla_X \mathcal{G}(\Theta,X)\)</span> and corresponding ‘optimal’
step-sizes using the Lipschitz coefficients of the gradients- which is the advantage of having a bi-smooth
optimization problem. Note that the objective <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, given <span class="math notranslate nohighlight">\(X\)</span> is separable in <span class="math notranslate nohighlight">\(\Theta_1 := (A,B,c)\)</span> and <span class="math notranslate nohighlight">\(\Theta_2 := (D,E,f)\)</span>.
Using scalar by matrix calculus</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
        \nabla_A \mathcal{G}(\Theta,X) = \Omega(A,B,c)X^T \in \mathbb{R}^{p \times h} \\
        \nabla_B \mathcal{G}(\Theta,X) = \Omega(A,B,c)U^T \in \mathbb{R}^{p \times n} \\
\nabla_c \mathcal{G}(\Theta,X) = \Omega(A,B,c)1_m \in \mathbb{R}^p
\end{cases},\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(\Omega(A,B,c) := \frac{1}{m}(AX + BU + c1_m^T - Y) \in \mathbb{R}^{p \times m}\)</span>. Hence we can show that a Lipschitz constant for the
gradient is given by</p>
<div class="math notranslate nohighlight">
\[L_{\Theta_1}(X) := \frac{1}{m} \max(m,\Vert X \Vert_2^2,\Vert U \Vert_2^2,\Vert XU^T \Vert_2),\]</div>
<p>and the ‘optimal’ step-size for gradient descent is then simply given by</p>
<div class="math notranslate nohighlight">
\[\alpha_{\Theta_1}(X) := \frac{1}{L_{\Theta}(X)}.\]</div>
<p>Regarding the gradient with respect to <span class="math notranslate nohighlight">\(\Theta_2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
        \nabla_D \mathcal{G}(\Theta,X) = \Omega(D,E,f,\Lambda)X^T \in \mathbb{R}^{h \times h} \\
        \nabla_E \mathcal{G}(\Theta,X) = \Omega(D,E,f,\Lambda)U^T \in \mathbb{R}^{h \times n} \\
\nabla_f \mathcal{G}(\Theta,X) = \Omega(D,E,f,\Lambda)1_m \in \mathbb{R}^h
\end{cases},\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(\Omega(D,E,f,\Lambda) := \frac{\Lambda}{m}\bigg((DX + EU + f1_m^T)_+ - X \bigg) \in \mathbb{R}^{h \times m}\)</span>, we can show that a Lipschitz constant for the
gradient is</p>
<div class="math notranslate nohighlight">
\[L_{\Theta_2}(X) := \frac{\lambda_{\text{max}}}{m} \max(m,\Vert X \Vert_2^2,\Vert U \Vert_2^2,\Vert X \Vert_2 \Vert U \Vert_2),\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_{\text{max}} = \text{max}_{j \in \{1,\cdots,h\}} \lambda_j\)</span>. We can then similarly define an ‘optimal’ step-size <span class="math notranslate nohighlight">\(\alpha \Theta_2\)</span>.
We have that</p>
<div class="math notranslate nohighlight">
\[\nabla_X \mathcal{G}(\Theta,X) = \frac{1}{m} \bigg\{ A^T(AX + BU + c1_m^T) + (\Lambda - \Lambda D - D^T \Lambda)X + D^T \Lambda (DX + EU + f1_m^T)_+ - \Lambda(EU+f1_m^T) \bigg\}.\]</div>
<p>A Lipschitz constant for this gradient is</p>
<div class="math notranslate nohighlight">
\[L_X(\Theta) = \frac{1}{m}(\Vert A^TA + \Lambda - \Lambda D + D^T\Lambda \Vert_2 + \lambda_{\text{max}} \Vert D \Vert_2^2).\]</div>
<p>We can then take the step-size <span class="math notranslate nohighlight">\(\alpha_X(\Theta) = \frac{1}{L_X(\Theta)}\)</span>. We propose the following block coordinate projected
gradient scheme (BC-gradient) to  nd a candidate solution to :eq:<a href="#id2"><span class="problematic" id="id3">`</span></a>eq_7. We denote compactly the convex set</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}_{\Theta} := \{\Theta \vert \Lambda + A^TA - (\Lambda D + D^T \Lambda) \in \mathbb{s}_+^h, \Vert D \Vert_2 \leq 1 - \epsilon \}\]</div>
<p>and <span class="math notranslate nohighlight">\(\mathcal{P}_{\mathcal{S}_{\Theta}}\)</span> the corresponding convex projection</p>
<p><strong>for</strong> <span class="math notranslate nohighlight">\(k = 1, 2, \cdots\)</span> <strong>do</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \Theta^k &amp;= \mathcal{P}_{\mathcal{S}_{\Theta}}\bigg(\Theta^k - \alpha_{\Theta}(X^{k-1}) \nabla_{\Theta} \mathcal{G}(\Theta^{k-1},X^{k-1}) \bigg) \\
    X^k &amp;= \bigg(X^{k-1} - \alpha_X(\Theta^k) \nabla_X \mathcal{G}(\Theta^k,X^{k-1}) \bigg)
\end{align}\end{split}\]</div>
<p><strong>end</strong></p>
</div>
<div class="section" id="bi-convexity-of-the-loss-function">
<h2>Bi-Convexity of the Loss function<a class="headerlink" href="#bi-convexity-of-the-loss-function" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
<div class="section" id="id4">
<h2>Gradient Descents<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
<div class="section" id="code-for-calculating-gradient-descent">
<h2>Code for calculating Gradient Descent<a class="headerlink" href="#code-for-calculating-gradient-descent" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-utilities.GradientDescents"></span><dl class="function">
<dt id="utilities.GradientDescents.gradient_descent_theta">
<code class="sig-prename descclassname">utilities.GradientDescents.</code><code class="sig-name descname">gradient_descent_theta</code><span class="sig-paren">(</span><em class="sig-param">theta</em>, <em class="sig-param">X</em>, <em class="sig-param">U</em>, <em class="sig-param">Y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utilities/GradientDescents.html#gradient_descent_theta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utilities.GradientDescents.gradient_descent_theta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the gradient of theta</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – a dictionary</p></li>
<li><p><strong>X</strong> – hidden variables</p></li>
<li><p><strong>U</strong> – input data</p></li>
<li><p><strong>Y</strong> – output data</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>grad_theta: dictionary containing gradients of elemnts in theta</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">IDL</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="bi_convex_formulation.html">Formulation for IDL</a></li>
<li class="toctree-l1"><a class="reference internal" href="learning.html">Learning Process</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradient Descents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#block-coordinate-descent-and-first-order-methods">Block coordinate descent and first order methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bi-convexity-of-the-loss-function">Bi-Convexity of the Loss function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">Gradient Descents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#code-for-calculating-gradient-descent">Code for calculating Gradient Descent</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dual_ascents.html">Dual Ascents</a></li>
<li class="toctree-l1"><a class="reference internal" href="prediction.html">Predicting</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification_operational_example.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="citing.html">Citing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="learning.html" title="previous chapter">Learning Process</a></li>
      <li>Next: <a href="dual_ascents.html" title="next chapter">Dual Ascents</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/sections/gradient_descents.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>