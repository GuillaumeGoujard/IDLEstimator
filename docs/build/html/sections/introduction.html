
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap-theme.min.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
    <title>Introduction &#8212; IDL 0.9 documentation</title>
    <link rel="stylesheet" href="../_static/guzzle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Formulation for IDL" href="bi_convex_formulation.html" />
    <link rel="prev" title="Implicit Deep Learning Solver" href="../index.html" />
  
   

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="bi_convex_formulation.html" title="Formulation for IDL"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Implicit Deep Learning Solver"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">IDL 0.9 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column">
    <div class="sphinxsidebar">
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <h2>Contents</h2>
    <div class="sidebar-localtoc">
      <ul>
<li><a class="reference internal" href="#">Introduction</a><ul>
<li><a class="reference internal" href="#id1">1.1 Implicit Deep Learning</a></li>
<li><a class="reference internal" href="#notation-and-definitions">1.2 Notation and definitions</a></li>
<li><a class="reference internal" href="#well-posedness">2 Well-posedness</a></li>
<li><a class="reference internal" href="#loss-functions">Loss Functions</a></li>
<li><a class="reference internal" href="#description-of-the-learning-process">Description of the learning process</a></li>
<li><a class="reference internal" href="#description-of-the-prediction-process">Description of the prediction process</a></li>
<li><a class="reference internal" href="#setup">Setup</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
</div>
  <h4>Previous topic</h4>
  <p class="topless"><a href="../index.html"
                        title="previous chapter">Implicit Deep Learning Solver</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="bi_convex_formulation.html"
                        title="next chapter">Formulation for IDL</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/sections/introduction.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <div id="main-search">
      <form class="form-inline" action="../search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
      
    </div>
  </div>
        <div id="right-column">
          
          <div role="navigation" aria-label="breadcrumbs navigation">
            <ol class="breadcrumb">
              <li><a href="../index.html">Docs</a></li>
              
              <li>Introduction</li>
            </ol>
          </div>
          
          <div class="document clearer body">
            
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>This package can be used to fit an Implicit Deep Learning (IDL) model for regression
and classification purpose.</p>
<p>The IDL.fit function estimates a vector of parameters by applying successively
gradient descents (see more at <a class="reference internal" href="gradient_descents.html#gradient-descents"><span class="std std-ref">Gradient Descents</span></a>) and dual ascent
(see more at <a class="reference internal" href="dual_ascents.html#dual-ascents"><span class="std std-ref">Dual Ascents</span></a>).</p>
<div class="section" id="id1">
<h2>1.1 Implicit Deep Learning<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Given an input <span class="math notranslate nohighlight">\(u \in \mathbb{R}^n\)</span>, where n denotes the number of features,
we define the implicit deep learning prediction rule <span class="math notranslate nohighlight">\(\hat{y}(u) \in \mathbb{R}^n\)</span> with ReLU activation</p>
<div class="math notranslate nohighlight" id="equation-eq-1">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
    \hat{y}(u) &amp;= Ax + Bu + c \\
    x &amp;= (Dx + Eu + f)_+,
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((.)_+ := \text{max}(0,.)\)</span> is ReLU activation, <span class="math notranslate nohighlight">\(x \in \mathbb{R}^h\)</span> is called the hidden variable
(h is the number of hidden features), <span class="math notranslate nohighlight">\(\Theta := (A,B,c,D,E,f)\)</span> are matrices and vectors of appropriate size, they define the
parameters of the model. The hidden variable <span class="math notranslate nohighlight">\(x\)</span> is implicit in the sense that there is in general no analytical
formula for it, this is different from classic deep learning for which, given the model parameters, the hidden
variables can be computed explicitly via propagation trough the network.</p>
</div>
<div class="section" id="notation-and-definitions">
<h2>1.2 Notation and definitions<a class="headerlink" href="#notation-and-definitions" title="Permalink to this headline">¶</a></h2>
<p>We denote <span class="math notranslate nohighlight">\(\Vert . \Vert\)</span> the eucledian norm, <span class="math notranslate nohighlight">\(\Vert . \Vert_2\)</span> the corresponding norm (i.e. the spectral norm) and
<span class="math notranslate nohighlight">\(\Vert . \Vert_F\)</span> the Frobenius norm. <span class="math notranslate nohighlight">\(\mathbb{R}_+^n\)</span> denotes the positive orthant of the vector space <span class="math notranslate nohighlight">\(\mathbb{R}^n, \mathbb{S}^n\)</span>
the set of real symmetric matrices of size <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(\mathbb{S}_+^n\)</span> the cone of positive semi-definite matrices of size <span class="math notranslate nohighlight">\(n\)</span>. The transpose of a matrix or
vector is denoted <span class="math notranslate nohighlight">\(.^T\)</span> and elementwise product is denoted <span class="math notranslate nohighlight">\(\odot\)</span>. Given a differentiable function <span class="math notranslate nohighlight">\(f\)</span> from <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times p}\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>
we define the scalar by matrix partial derivative in denominator layout convention as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial f}{\partial A} = \nabla_A f = \begin{bmatrix}
        \frac{\partial f}{\partial A_{1,1}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{1,p}} \\
        \vdots &amp; \ddots &amp; \vdots \\
        \frac{\partial f}{\partial A_{n,1}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{n,p}}
    \end{bmatrix}
    \in \mathbb{R}^{n \times p}.\end{split}\]</div>
<p>We say that a function <span class="math notranslate nohighlight">\((x,y) \rightarrow f(x,y)\)</span> with seperable domain of definition <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span> is bi-convex in <span class="math notranslate nohighlight">\((x,y)\)</span>,
if for all <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>, the function <span class="math notranslate nohighlight">\(y \rightarrow f(x,y)\)</span> is convex and for all <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> the function <span class="math notranslate nohighlight">\(x \rightarrow f(x,y)\)</span> is convex.
We say that a function is smooth if it is differentiable and its gradient is Lipschitz continious. We say that <span class="math notranslate nohighlight">\(f\)</span> is bi-smooth if it is smooth in <span class="math notranslate nohighlight">\(x\)</span> given <span class="math notranslate nohighlight">\(y\)</span> and
vice-versa. An example of bi-smooth and bi-convex function is <span class="math notranslate nohighlight">\((x,A) \rightarrow x^TAx, A \in \mathbb{S}_+^n\)</span>.</p>
</div>
<div class="section" id="well-posedness">
<h2>2 Well-posedness<a class="headerlink" href="#well-posedness" title="Permalink to this headline">¶</a></h2>
<p>We say that matrix <span class="math notranslate nohighlight">\(D\)</span> is well-posed for <a class="reference internal" href="#equation-eq-1">(1)</a> if there exists a unique solution <span class="math notranslate nohighlight">\(x = (Dx + \delta)_+ \forall \delta \in \mathbb{R}^h\)</span>.
Using the fact that ReLU is 1-Lipschitz we have for <span class="math notranslate nohighlight">\(x_1,x_2 \in \mathbb{R}^h\)</span></p>
<div class="math notranslate nohighlight">
\[\Vert (Dx_1 + \delta)_+ - (Dx_2 + \delta)_+ \Vert \leq \Vert D(x_1 -x_2) \Vert \leq \Vert D \Vert_2 \Vert x_1 -x_2 \Vert.\]</div>
<p>If <span class="math notranslate nohighlight">\(\Vert D \Vert_2 &lt; 1\)</span> we have that the map <span class="math notranslate nohighlight">\(x \rightarrow (Dx + \delta)_+\)</span> is a strict contraction. In that case, Banach’s contraction
mapping theorem applies, showing that the equation <span class="math notranslate nohighlight">\(x = (Dx + \delta)_+\)</span> has a unique solution. In that case, a solution <span class="math notranslate nohighlight">\(x\)</span> can be computed via the
Picard iterations</p>
<div class="math notranslate nohighlight">
\[x^{k+1} = (Dx + \delta), k = 1,2, \cdots.\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\Vert D \Vert_2 &lt; 1\)</span> is only a sufficient condition for well-posedness. Nevertheless this is the only condition
we will consider in this article.</p>
</div>
<div class="section" id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>** section 3.2 equation 4 ** + Classification loss
(see more at <a class="reference internal" href="learning.html#learning"><span class="std std-ref">Learning Process</span></a>)</p>
</div>
<div class="section" id="description-of-the-learning-process">
<h2>Description of the learning process<a class="headerlink" href="#description-of-the-learning-process" title="Permalink to this headline">¶</a></h2>
<p>(see more at <a class="reference internal" href="bi_convex_formulation.html#formulation"><span class="std std-ref">Formulation for IDL</span></a>)</p>
</div>
<div class="section" id="description-of-the-prediction-process">
<h2>Description of the prediction process<a class="headerlink" href="#description-of-the-prediction-process" title="Permalink to this headline">¶</a></h2>
<p>(see more at <a class="reference internal" href="prediction.html#prediction"><span class="std std-ref">Predicting</span></a>)</p>
</div>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
<p>The package is compatible with Python version 3 or higher only.
The user is expected to have installed cvxpy before running the package.
Go to … for more information.</p>
<ol class="arabic simple">
<li><p>Switch to a proper directory and then type:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="o">+</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/...</span>
</pre></div>
</div>
</div>
</div>


          </div>
            
  <div class="footer-relations">
    
      <div class="pull-left">
        <a class="btn btn-default" href="../index.html" title="previous chapter (use the left arrow)">Implicit Deep Learning Solver</a>
      </div>
    
      <div class="pull-right">
        <a class="btn btn-default" href="bi_convex_formulation.html" title="next chapter (use the right arrow)">Formulation for IDL</a>
      </div>
    </div>
    <div class="clearer"></div>
  
        </div>
        <div class="clearfix"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="bi_convex_formulation.html" title="Formulation for IDL"
             >next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Implicit Deep Learning Solver"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">IDL 0.9 documentation</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="../_static/js/bootstrap.js"></script>
  <div class="footer">
    &copy; Copyright . Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>